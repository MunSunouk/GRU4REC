{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir20RCtHfMq0"
      },
      "source": [
        "import time\r\n",
        "from pathlib import Path\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import torch.optim as optim\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_WxoMcUfncZ",
        "outputId": "fc252963-2dc2-48b2-cc86-cfdfdad37df6"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDrQXRMyfu5y",
        "outputId": "25a21b25-d2af-4e3c-ccaa-aa8cc6770604"
      },
      "source": [
        "cd /content/gdrive/My Drive/버즈니"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/버즈니\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUzuFHEgn0hd"
      },
      "source": [
        "train_dataset = pd.read_csv('./recommendation_dataset/user_train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "KKytANgtn3s9",
        "outputId": "65a5e3a4-5a6b-4601-a2e8-40b60eeb122e"
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>user_id</th>\n",
              "      <th>product_id</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1531461682-93ee1a59-e658-4ef5-b4a9-fc8332471e38</td>\n",
              "      <td>964b882e9974101838e0f4907385754c7a76c67804c1b7...</td>\n",
              "      <td>20180714000144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1531461682-93ee1a59-e658-4ef5-b4a9-fc8332471e38</td>\n",
              "      <td>d4cf98cf780d973a69060b5e75c7e603a36f4d4ec681c1...</td>\n",
              "      <td>20180714000149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1531461682-93ee1a59-e658-4ef5-b4a9-fc8332471e38</td>\n",
              "      <td>7fddb5bb71072cc2b96ff5cc2aa7a16d961ff1267c0873...</td>\n",
              "      <td>20180714000240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1531461682-93ee1a59-e658-4ef5-b4a9-fc8332471e38</td>\n",
              "      <td>2586a3ed1bffb041ee8094fe3b126ed5d5c722928cb22c...</td>\n",
              "      <td>20180714000305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1531461682-93ee1a59-e658-4ef5-b4a9-fc8332471e38</td>\n",
              "      <td>35ea063d443ec3691c2e77c3a76e0c9006cf5d10d9cd26...</td>\n",
              "      <td>20180714000406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47314</th>\n",
              "      <td>4999</td>\n",
              "      <td>1531633713-7809e3f0-3ff4-4618-b46b-802c1e696826</td>\n",
              "      <td>8aa0dda8cf278c229a03e3aff1ce01eb3a1f2755b8bb46...</td>\n",
              "      <td>20180715235041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47315</th>\n",
              "      <td>4999</td>\n",
              "      <td>1531633713-7809e3f0-3ff4-4618-b46b-802c1e696826</td>\n",
              "      <td>8aa0dda8cf278c229a03e3aff1ce01eb3a1f2755b8bb46...</td>\n",
              "      <td>20180715235059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47316</th>\n",
              "      <td>4999</td>\n",
              "      <td>1531633713-7809e3f0-3ff4-4618-b46b-802c1e696826</td>\n",
              "      <td>8aa0dda8cf278c229a03e3aff1ce01eb3a1f2755b8bb46...</td>\n",
              "      <td>20180715235125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47317</th>\n",
              "      <td>4999</td>\n",
              "      <td>1531633713-7809e3f0-3ff4-4618-b46b-802c1e696826</td>\n",
              "      <td>8aa0dda8cf278c229a03e3aff1ce01eb3a1f2755b8bb46...</td>\n",
              "      <td>20180715235146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47318</th>\n",
              "      <td>4999</td>\n",
              "      <td>1531633713-7809e3f0-3ff4-4618-b46b-802c1e696826</td>\n",
              "      <td>90b51c8cf0c55465219124623f73d9aa3584b65b411012...</td>\n",
              "      <td>20180715235348</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>47319 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  ...            data\n",
              "0               0  ...  20180714000144\n",
              "1               0  ...  20180714000149\n",
              "2               0  ...  20180714000240\n",
              "3               0  ...  20180714000305\n",
              "4               0  ...  20180714000406\n",
              "...           ...  ...             ...\n",
              "47314        4999  ...  20180715235041\n",
              "47315        4999  ...  20180715235059\n",
              "47316        4999  ...  20180715235125\n",
              "47317        4999  ...  20180715235146\n",
              "47318        4999  ...  20180715235348\n",
              "\n",
              "[47319 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhbU8hM3ffY0"
      },
      "source": [
        "class SessionDataset:\r\n",
        "    def __init__(self, path, sep='\\t', session_key='user_id', item_key='product_id', time_key='data', n_samples=-1, itemmap=None, time_sort=False):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            path: path of the csv file\r\n",
        "            sep: separator for the csv\r\n",
        "            session_key, item_key, time_key: name of the fields corresponding to the sessions, items, time\r\n",
        "            n_samples: the number of samples to use. If -1, use the whole dataset.\r\n",
        "            itemmap: mapping between item IDs and item indices\r\n",
        "            time_sort: whether to sort the sessions by time or not\r\n",
        "        \"\"\"\r\n",
        "        self.df = pd.read_csv(path, names=[session_key, item_key, time_key],low_memory=False)\r\n",
        "        self.session_key = session_key\r\n",
        "        self.item_key = item_key\r\n",
        "        self.time_key = time_key\r\n",
        "        self.time_sort = time_sort\r\n",
        "        \r\n",
        "        # sampling\r\n",
        "        if n_samples > 0: self.df = self.df[:n_samples] \r\n",
        "        # Add item indices\r\n",
        "        self.add_item_indices(itemmap=itemmap)\r\n",
        "        \"\"\"\r\n",
        "        Sort the df by time, and then by session ID. That is, df is sorted by session ID and\r\n",
        "        clicks within a session are next to each other, where the clicks within a session are time-ordered.\r\n",
        "        \"\"\"\r\n",
        "        self.df.sort_values([session_key, time_key], inplace=True)\r\n",
        "         \r\n",
        "        self.click_offsets = self.get_click_offsets()\r\n",
        "        self.session_idx_arr = self.order_session_idx()\r\n",
        "        \r\n",
        "        \r\n",
        "    def get_click_offsets(self):\r\n",
        "        \"\"\"\r\n",
        "        Return the offsets of the beginning clicks of each session IDs,\r\n",
        "        where the offset is calculated against the first click of the first session ID.\r\n",
        "        \"\"\"\r\n",
        "        offsets = np.zeros(self.df[self.session_key].nunique() + 1, dtype=np.int32)\r\n",
        "        # group & sort the df by session_key and get the offset values\r\n",
        "        offsets[1:] = self.df.groupby(self.session_key).size().cumsum()\r\n",
        "\r\n",
        "        return offsets\r\n",
        "    \r\n",
        "\r\n",
        "    def order_session_idx(self):\r\n",
        "        \"\"\" Order the session indices \"\"\"\r\n",
        "        if self.time_sort:\r\n",
        "            # starting time for each sessions, sorted by session IDs\r\n",
        "            sessions_start_time = self.df.groupby(self.session_key)[self.time_key].min().values\r\n",
        "            # order the session indices by session starting times\r\n",
        "            session_idx_arr = np.argsort(sessions_start_time)\r\n",
        "        else:\r\n",
        "            session_idx_arr = np.arange(self.df[self.session_key].nunique())\r\n",
        "\r\n",
        "        return session_idx_arr\r\n",
        "    \r\n",
        "    \r\n",
        "    def add_item_indices(self, itemmap=None):\r\n",
        "        \"\"\" \r\n",
        "        Add item index column named \"item_idx\" to the df\r\n",
        "        Args:\r\n",
        "            itemmap (pd.DataFrame): mapping between the item Ids and indices\r\n",
        "        \"\"\"\r\n",
        "        if itemmap is None:\r\n",
        "            item_ids = self.df[self.item_key].unique()  # unique item ids\r\n",
        "            item2idx = pd.Series(data=np.arange(len(item_ids)),\r\n",
        "                                 index=item_ids)\r\n",
        "            itemmap = pd.DataFrame({self.item_key:item_ids,\r\n",
        "                                   'item_idx':item2idx[item_ids].values})\r\n",
        "        \r\n",
        "        self.itemmap = itemmap\r\n",
        "        self.df = pd.merge(self.df, self.itemmap, on=self.item_key, how='inner')\r\n",
        "        \r\n",
        "    \r\n",
        "    @property    \r\n",
        "    def items(self):\r\n",
        "        return self.itemmap.product_id.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eUG3GO6jyTt"
      },
      "source": [
        "class SessionDataLoader:\r\n",
        "    def __init__(self, dataset, batch_size=50):\r\n",
        "        \"\"\"\r\n",
        "        A class for creating session-parallel mini-batches.\r\n",
        "        Args:\r\n",
        "             dataset (SessionDataset): the session dataset to generate the batches from\r\n",
        "             batch_size (int): size of the batch\r\n",
        "        \"\"\"\r\n",
        "        self.dataset = dataset\r\n",
        "        self.batch_size = batch_size\r\n",
        "        \r\n",
        "        \r\n",
        "    def __iter__(self):\r\n",
        "        \"\"\" Returns the iterator for producing session-parallel training mini-batches.\r\n",
        "        Yields:\r\n",
        "            input (B,): torch.FloatTensor. Item indices that will be encoded as one-hot vectors later.\r\n",
        "            target (B,): a Variable that stores the target item indices\r\n",
        "            masks: Numpy array indicating the positions of the sessions to be terminated\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        # initializations\r\n",
        "        df = self.dataset.df\r\n",
        "        click_offsets = self.dataset.click_offsets\r\n",
        "        session_idx_arr = self.dataset.session_idx_arr\r\n",
        "\r\n",
        "        iters = np.arange(self.batch_size)\r\n",
        "        maxiter = iters.max()\r\n",
        "        start = click_offsets[session_idx_arr[iters]]\r\n",
        "        end = click_offsets[session_idx_arr[iters] + 1]\r\n",
        "        mask = [] # indicator for the sessions to be terminated\r\n",
        "        finished = False\r\n",
        "        \r\n",
        "        item_idx = df.item_idx.values\r\n",
        "\r\n",
        "        while not finished:\r\n",
        "            minlen = (end - start).min()\r\n",
        "            # Item indices(for embedding) for clicks where the first sessions start\r\n",
        "            idx_target = item_idx[start]\r\n",
        "            for i in range(minlen - 1):\r\n",
        "                # Build inputs & targets\r\n",
        "                idx_input = idx_target\r\n",
        "                idx_target = item_idx[start + i + 1]\r\n",
        "                input = torch.LongTensor(idx_input)\r\n",
        "                target = torch.LongTensor(idx_target)\r\n",
        "                # stop flushing the hidden state after the first step\r\n",
        "                if i == 1: mask = []\r\n",
        "                yield input, target, mask\r\n",
        "                \r\n",
        "            # click indices where a particular session meets second-to-last element\r\n",
        "            start = start + (minlen - 1)\r\n",
        "            # see if how many sessions should terminate\r\n",
        "            mask = np.arange(len(iters))[(end - start) <= 1]\r\n",
        "            for idx in mask:\r\n",
        "                maxiter += 1\r\n",
        "                if maxiter >= len(click_offsets) - 1:\r\n",
        "                    finished = True\r\n",
        "                    break\r\n",
        "                # update the next starting/ending point\r\n",
        "                iters[idx] = maxiter\r\n",
        "                start[idx] = click_offsets[session_idx_arr[maxiter]]\r\n",
        "                end[idx] = click_offsets[session_idx_arr[maxiter] + 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhqj_9l3f447"
      },
      "source": [
        "class Optimizer:\r\n",
        "    def __init__(self, params, optimizer_type='Adagrad', lr=.05,\r\n",
        "                 momentum=0, weight_decay=0, eps=1e-6):\r\n",
        "        '''\r\n",
        "        An abstract optimizer class for handling various kinds of optimizers.\r\n",
        "        You can specify the optimizer type and related parameters as you want.\r\n",
        "        Usage is exactly the same as an instance of torch.optim\r\n",
        "        Args:\r\n",
        "            params: torch.nn.Parameter. The NN parameters to optimize\r\n",
        "            optimizer_type: type of the optimizer to use\r\n",
        "            lr: learning rate\r\n",
        "            momentum: momentum, if needed\r\n",
        "            weight_decay: weight decay, if needed. Equivalent to L2 regulariztion.\r\n",
        "            eps: eps parameter, if needed.\r\n",
        "        '''\r\n",
        "        if optimizer_type == 'RMSProp':\r\n",
        "            self.optimizer = optim.RMSprop(params, lr=lr,\r\n",
        "                                           eps=eps,\r\n",
        "                                           weight_decay=weight_decay,\r\n",
        "                                           momentum=momentum)\r\n",
        "        elif optimizer_type == 'Adagrad':\r\n",
        "            self.optimizer = optim.Adagrad(params, lr=lr,\r\n",
        "                                           weight_decay=weight_decay)\r\n",
        "        elif optimizer_type == 'Adadelta':\r\n",
        "            self.optimizer = optim.Adadelta(params,\r\n",
        "                                            lr=lr,\r\n",
        "                                            eps=eps,\r\n",
        "                                            weight_decay=weight_decay)\r\n",
        "        elif optimizer_type == 'Adam':\r\n",
        "            self.optimizer = optim.Adam(params,\r\n",
        "                                        lr=lr,\r\n",
        "                                        eps=eps,\r\n",
        "                                        weight_decay=weight_decay)\r\n",
        "\r\n",
        "        elif optimizer_type == 'SparseAdam':\r\n",
        "            self.optimizer = optim.SparseAdam(params,\r\n",
        "                                              lr=lr,\r\n",
        "                                              eps=eps)\r\n",
        "        elif optimizer_type == 'SGD':\r\n",
        "            self.optimizer = optim.SGD(params,\r\n",
        "                                       lr=lr,\r\n",
        "                                       momentum=momentum,\r\n",
        "                                       weight_decay=weight_decay)\r\n",
        "        else:\r\n",
        "            raise NotImplementedError\r\n",
        "                 \r\n",
        "    def zero_grad(self):\r\n",
        "        self.optimizer.zero_grad()\r\n",
        "    \r\n",
        "    def step(self):\r\n",
        "        self.optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mVGJ0cSgOEZ"
      },
      "source": [
        "class GRU(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1,\r\n",
        "                 p_dropout_hidden=0, p_dropout_input=0, batch_size=50, use_cuda=True):\r\n",
        "        '''\r\n",
        "        The GRU layer used for the whole GRU4REC model.\r\n",
        "        Args:\r\n",
        "            input_size (int): input layer dimension\r\n",
        "            hidden_size (int): hidden layer dimension\r\n",
        "            output_size (int): output layer dimension. Equivalent to the number of classes\r\n",
        "            num_layers (int): the number of GRU layers\r\n",
        "            p_dropout_hidden (float): dropout probability for the GRU hidden layers\r\n",
        "            p_dropout_input (float): dropout probability for the GRU input layer\r\n",
        "            batch_size (int): size of the training batch.(required for producing one-hot encodings efficiently)\r\n",
        "            use_cuda (bool): whether to use cuda or not\r\n",
        "            training (bool): whether to set the GRU module to training mode or not. If false, parameters will not be updated.\r\n",
        "        '''\r\n",
        "\r\n",
        "        super().__init__()\r\n",
        "        self.input_size = input_size\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.output_size = output_size\r\n",
        "        self.num_layers = num_layers\r\n",
        "        self.p_dropout_input = p_dropout_input\r\n",
        "        self.dropout_hidden = nn.Dropout(p_dropout_hidden)\r\n",
        "\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.use_cuda = use_cuda\r\n",
        "        self.device = torch.device('cuda' if use_cuda else 'cpu')\r\n",
        "\r\n",
        "        self.onehot_buffer = self.init_emb()  # the buffer where the one-hot encodings will be produced from\r\n",
        "        self.h2o = nn.Linear(hidden_size, output_size)\r\n",
        "        self.tanh = nn.Tanh()\r\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, dropout=p_dropout_hidden if num_layers > 1 else 0)\r\n",
        "        \r\n",
        "        self = self.to(self.device)\r\n",
        "        \r\n",
        "\r\n",
        "    def forward(self, input, target, hidden):\r\n",
        "        '''\r\n",
        "        Args:\r\n",
        "            input (B,): a batch of item indices from a session-parallel mini-batch.\r\n",
        "            target (B,): torch.LongTensor of next item indices from a session-parallel mini-batch.\r\n",
        "            \r\n",
        "        Returns:\r\n",
        "            logit (B,C): Variable that stores the logits for the next items in the session-parallel mini-batch\r\n",
        "            hidden: GRU hidden state\r\n",
        "        '''\r\n",
        "        embedded = self.onehot_encode(input)\r\n",
        "        if self.training and self.p_dropout_input > 0: embedded = self.input_dropout(embedded)\r\n",
        "        embedded = embedded.unsqueeze(0)  # (1,B,C)\r\n",
        "\r\n",
        "        # Go through the GRU layer\r\n",
        "        output, hidden = self.gru(embedded, hidden)  # (num_layers,B,H)\r\n",
        "        output = output.view(-1, output.size(-1))  # (B,H)\r\n",
        "        output = self.dropout_hidden(output) # hidden layer dropout\r\n",
        "        logit = self.tanh(self.h2o(output))  # (B,C)\r\n",
        "\r\n",
        "        return logit, hidden\r\n",
        "    \r\n",
        "    \r\n",
        "    def input_dropout(self, input):\r\n",
        "        p_drop = torch.Tensor(input.size(0), 1).fill_(1 - self.p_dropout_input)  # (B,1)\r\n",
        "        mask = torch.bernoulli(p_drop).expand_as(input)/(1-self.p_dropout_input) # (B,C)\r\n",
        "        mask = mask.to(self.device)\r\n",
        "        input = input * mask  # (B,C)\r\n",
        "        \r\n",
        "        return input\r\n",
        "        \r\n",
        "\r\n",
        "    def init_emb(self):\r\n",
        "        '''\r\n",
        "        Initialize the one_hot embedding buffer, which will be used for producing the one-hot embeddings efficiently\r\n",
        "        '''\r\n",
        "        onehot_buffer = torch.FloatTensor(self.batch_size, self.output_size)\r\n",
        "        onehot_buffer = onehot_buffer.to(self.device)\r\n",
        "\r\n",
        "        return onehot_buffer\r\n",
        "    \r\n",
        "    \r\n",
        "    def onehot_encode(self, input):\r\n",
        "        \"\"\"\r\n",
        "        Returns a one-hot vector corresponding to the input\r\n",
        "        Args:\r\n",
        "            input (B,): torch.LongTensor of item indices\r\n",
        "            buffer (B,output_size): buffer that stores the one-hot vector\r\n",
        "        Returns:\r\n",
        "            one_hot (B,C): torch.FloatTensor of one-hot vectors\r\n",
        "        \"\"\"\r\n",
        "        \r\n",
        "        self.onehot_buffer.zero_()\r\n",
        "        index = input.view(-1,1)\r\n",
        "        one_hot = self.onehot_buffer.scatter_(1, index, 1)\r\n",
        "        \r\n",
        "        return one_hot\r\n",
        "\r\n",
        "    \r\n",
        "    def init_hidden(self):\r\n",
        "        '''\r\n",
        "        Initialize the hidden state of the GRU\r\n",
        "        '''\r\n",
        "        h0 = torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.device)\r\n",
        "\r\n",
        "        return h0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5FiVQ0KjIiA"
      },
      "source": [
        "def LossFunction(loss_type):\r\n",
        "    if loss_type == 'CrossEntropy':\r\n",
        "        loss_fn = SampledCrossEntropyLoss\r\n",
        "    elif loss_type == 'TOP1':\r\n",
        "        loss_fn = TOP1Loss\r\n",
        "    elif loss_type == 'BPR':\r\n",
        "        loss_fn = BPRLoss\r\n",
        "    elif loss_type == 'TOP1-max':\r\n",
        "        loss_fn = TOP1_max\r\n",
        "    elif loss_type == 'BPR-max':\r\n",
        "        loss_fn = BPR_max\r\n",
        "    else:\r\n",
        "        raise NotImplementedError\r\n",
        "    return loss_fn\r\n",
        "\r\n",
        "\r\n",
        "xe_loss = nn.CrossEntropyLoss()\r\n",
        "def SampledCrossEntropyLoss(logit):\r\n",
        "    \"\"\" CrossEntropyLoss with n_classes = batch_size = the number of samples in the session-parallel mini-batch \"\"\"\r\n",
        "    batch_size = logit.size(1)\r\n",
        "    target = torch.arange(batch_size).long().to(logit.device)\r\n",
        "    return xe_loss(logit, target)\r\n",
        "\r\n",
        "\r\n",
        "def BPRLoss(logit):\r\n",
        "    \"\"\"\r\n",
        "    Args:\r\n",
        "        logit (BxB): Variable that stores the logits for the items in the session-parallel mini-batch.\r\n",
        "                     Negative samples for a specific item are drawn from the other items in the\r\n",
        "                     session-parallel minibatch, as mentioned in the original GRU4REC paper.\r\n",
        "                     The first dimension corresponds to the batches, and the second dimension\r\n",
        "                     corresponds to sampled number of items to evaluate.\r\n",
        "    \"\"\"\r\n",
        "    # differences between the item scores\r\n",
        "    diff = logit.diag().view(-1, 1).expand_as(logit) - logit\r\n",
        "    # final loss\r\n",
        "    loss = -torch.mean(F.logsigmoid(diff))\r\n",
        "\r\n",
        "    return loss\r\n",
        "\r\n",
        "def BPR_max(logit) :\r\n",
        "    logit_softmax = F.softmax(logit, dim=1)\r\n",
        "    diff = logit.diag().view(-1, 1).expand_as(logit) - logit\r\n",
        "    loss = -torch.log(torch.mean(logit_softmax * torch.sigmoid(diff)))\r\n",
        "    return loss\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "    \r\n",
        "def TOP1Loss(logit):\r\n",
        "    \"\"\"\r\n",
        "    Args:\r\n",
        "        logit (BxB): Variable that stores the logits for the items in the session-parallel mini-batch.\r\n",
        "                     Negative samples for a specific item are drawn from the other items in the\r\n",
        "                     session-parallel minibatch, as mentioned in the original GRU4REC paper.\r\n",
        "                     The first dimension corresponds to the batches, and the second dimension\r\n",
        "                     corresponds to sampled number of items to evaluate.\r\n",
        "    \"\"\"\r\n",
        "    # differences between the item scores\r\n",
        "    diff = -(logit.diag().view(-1, 1).expand_as(logit) - logit)\r\n",
        "    # final loss\r\n",
        "    loss = F.sigmoid(diff).mean() + F.sigmoid(logit ** 2).mean()\r\n",
        "\r\n",
        "    return loss\r\n",
        "\r\n",
        "def TOP1_max(logit) :\r\n",
        "\r\n",
        "    logit_softmax = F.softmax(logit, dim=1)\r\n",
        "    diff = -(logit.diag().view(-1, 1).expand_as(logit) - logit)\r\n",
        "    loss = torch.mean(logit_softmax * (torch.sigmoid(diff) + torch.sigmoid(logit ** 2)))\r\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yba0bHq4jmsi"
      },
      "source": [
        "def get_recall(indices, targets):\r\n",
        "    \"\"\" Calculates the recall score for the given predictions and targets\r\n",
        "    Args:\r\n",
        "        indices (Bxk): torch.LongTensor. top-k indices predicted by the model.\r\n",
        "        targets (B): torch.LongTensor. actual target indices.\r\n",
        "    Returns:\r\n",
        "        recall (float): the recall score\r\n",
        "    \"\"\"\r\n",
        "    targets = targets.view(-1, 1).expand_as(indices)  # (Bxk)\r\n",
        "    hits = (targets == indices).nonzero()\r\n",
        "    if len(hits) == 0: return 0\r\n",
        "    n_hits = (targets == indices).nonzero()[:, :-1].size(0)\r\n",
        "    recall = n_hits / targets.size(0)\r\n",
        "    \r\n",
        "    return recall\r\n",
        "\r\n",
        "def get_precision(indices, targets) :\r\n",
        "    \"\"\" Calculates the precision score for the given predictions and targets\r\n",
        "        indices (Bxk): torch.LongTensor. top-k indices predicted by the model.\r\n",
        "        targets (B): torch.LongTensor. actual target indices.\r\n",
        "    Returns:\r\n",
        "        precision (float): the precision score\r\n",
        "    \"\"\"    \r\n",
        "    targets = targets.view(-1, 1).expand_as(indices)  # (Bxk)\r\n",
        "    hits = (targets == indices).nonzero()\r\n",
        "    if len(hits) == 0: return 0\r\n",
        "    n_hits = (targets == indices).nonzero()[:, :-1].size(0)\r\n",
        "    precision = n_hits / indices.size(-1)\r\n",
        "    return precision\r\n",
        "\r\n",
        "def get_f1score(recall,precision) :\r\n",
        "\r\n",
        "    f1score = 2 * (precision * recall / (precision + recall))\r\n",
        "    return f1score\r\n",
        "\r\n",
        "def get_mcc(indices, targets) :\r\n",
        "\r\n",
        "    targets = targets.view(-1, 1).expand_as(indices)  # (Bxk)\r\n",
        "    hits = (targets == indices).nonzero()\r\n",
        "    if len(hits) == 0: return 0\r\n",
        "    n_hits = (targets == indices).nonzero()[:, :-1].size(0)\r\n",
        "\r\n",
        "    targets.size(0) * indices.size(-1) *\r\n",
        "    mcc = \r\n",
        "\r\n",
        "def get_mrr(indices, targets):\r\n",
        "    \"\"\" Calculates the MRR score for the given predictions and targets\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        indices (Bxk): torch.LongTensor. top-k indices predicted by the model.\r\n",
        "        targets (B): torch.LongTensor. actual target indices.\r\n",
        "    Returns:\r\n",
        "        mrr (float): the mrr score\r\n",
        "    \"\"\"\r\n",
        "    targets = targets.view(-1,1).expand_as(indices)\r\n",
        "    # ranks of the targets, if it appears in your indices\r\n",
        "    hits = (targets == indices).nonzero()\r\n",
        "    if len(hits) == 0: return 0\r\n",
        "    ranks = hits[:, -1] + 1\r\n",
        "    ranks = ranks.float()\r\n",
        "    rranks = torch.reciprocal(ranks)  # reciprocal ranks\r\n",
        "    mrr = torch.sum(rranks).data / targets.size(0)\r\n",
        "    mrr = mrr.item()\r\n",
        "    \r\n",
        "    return mrr\r\n",
        "\r\n",
        "\r\n",
        "def evaluate(logits, targets, k=20):\r\n",
        "    \"\"\" Evaluates the model using Recall@K, MRR@K scores.\r\n",
        "    Args:\r\n",
        "        logits (B,C): torch.LongTensor. The predicted logit for the next items.\r\n",
        "        targets (B): torch.LongTensor. actual target indices.\r\n",
        "    Returns:\r\n",
        "        recall (float): the recall score\r\n",
        "        mrr (float): the mrr score\r\n",
        "    \"\"\"\r\n",
        "    _, indices = torch.topk(logits, k, -1)\r\n",
        "    recall = get_recall(indices, targets)\r\n",
        "    precision = get_precision(indices,targets)\r\n",
        "    mrr = get_mrr(indices, targets)\r\n",
        "\r\n",
        "    return recall, precision, mrr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuLZ0eWbjnbA"
      },
      "source": [
        "class GRU4REC:\r\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1,\r\n",
        "                 optimizer_type='Adagrad', lr=.01, weight_decay=0,\r\n",
        "                 momentum=0, eps=1e-6, loss_type='TOP1',\r\n",
        "                 clip_grad=-1, p_dropout_input=.0, p_dropout_hidden=.5,\r\n",
        "                 batch_size=50, use_cuda=True, time_sort=False, pretrained=None):\r\n",
        "        \"\"\" The GRU4REC model\r\n",
        "        Args:\r\n",
        "            input_size (int): dimension of the gru input variables\r\n",
        "            hidden_size (int): dimension of the gru hidden units\r\n",
        "            output_size (int): dimension of the gru output variables\r\n",
        "            num_layers (int): the number of layers in the GRU\r\n",
        "            optimizer_type (str): optimizer type for GRU weights\r\n",
        "            lr (float): learning rate for the optimizer\r\n",
        "            weight_decay (float): weight decay for the optimizer\r\n",
        "            momentum (float): momentum for the optimizer\r\n",
        "            eps (float): eps for the optimizer\r\n",
        "            loss_type (str): type of the loss function to use\r\n",
        "            clip_grad (float): clip the gradient norm at clip_grad. No clipping if clip_grad = -1\r\n",
        "            p_dropout_input (float): dropout probability for the input layer\r\n",
        "            p_dropout_hidden (float): dropout probability for the hidden layer\r\n",
        "            batch_size (int): mini-batch size\r\n",
        "            use_cuda (bool): whether you want to use cuda or not\r\n",
        "            time_sort (bool): whether to ensure the the order of sessions is chronological (default: False)\r\n",
        "            pretrained (modules.layer.GRU): pretrained GRU layer, if it exists (default: None)\r\n",
        "        \"\"\"\r\n",
        "        \r\n",
        "        # Initialize the GRU Layer\r\n",
        "        self.input_size = input_size\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.output_size = output_size\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.use_cuda = use_cuda\r\n",
        "        self.device = torch.device('cuda' if use_cuda else 'cpu')\r\n",
        "        if pretrained is None:\r\n",
        "            self.gru = GRU(input_size, hidden_size, output_size, num_layers,\r\n",
        "                           p_dropout_input=p_dropout_input,\r\n",
        "                           p_dropout_hidden=p_dropout_hidden,\r\n",
        "                           batch_size=batch_size,\r\n",
        "                           use_cuda=use_cuda)\r\n",
        "        else:\r\n",
        "            self.gru = pretrained\r\n",
        "\r\n",
        "        # Initialize the optimizer\r\n",
        "        self.optimizer_type = optimizer_type\r\n",
        "        self.weight_decay = weight_decay\r\n",
        "        self.momentum = momentum\r\n",
        "        self.lr = lr\r\n",
        "        self.eps = eps\r\n",
        "        self.optimizer = Optimizer(self.gru.parameters(),\r\n",
        "                                   optimizer_type=optimizer_type,\r\n",
        "                                   lr=lr,\r\n",
        "                                   weight_decay=weight_decay,\r\n",
        "                                   momentum=momentum,\r\n",
        "                                   eps=eps)\r\n",
        "\r\n",
        "        # Initialize the loss function\r\n",
        "        self.loss_type = loss_type\r\n",
        "        self.loss_fn = LossFunction(loss_type)\r\n",
        "\r\n",
        "        # gradient clipping(optional)\r\n",
        "        self.clip_grad = clip_grad \r\n",
        "\r\n",
        "        # etc\r\n",
        "        self.time_sort = time_sort\r\n",
        "        \r\n",
        "        \r\n",
        "    def run_epoch(self, dataset, k=20, training=True):\r\n",
        "        \"\"\" Run a single training epoch \"\"\"\r\n",
        "        start_time = time.time()\r\n",
        "        \r\n",
        "        # initialize\r\n",
        "        losses = []\r\n",
        "        recalls = []\r\n",
        "        precisions = []\r\n",
        "        mrrs = []\r\n",
        "        optimizer = self.optimizer\r\n",
        "        hidden = self.gru.init_hidden()\r\n",
        "        if not training:\r\n",
        "            self.gru.eval()\r\n",
        "        device = self.device\r\n",
        "        \r\n",
        "        def reset_hidden(hidden, mask):\r\n",
        "            \"\"\"Helper function that resets hidden state when some sessions terminate\"\"\"\r\n",
        "            if len(mask) != 0:\r\n",
        "                hidden[:, mask, :] = 0\r\n",
        "            \r\n",
        "            return hidden\r\n",
        "\r\n",
        "        # Start the training loop\r\n",
        "        loader = SessionDataLoader(dataset, batch_size=self.batch_size)\r\n",
        "\r\n",
        "        for input, target, mask in loader:\r\n",
        "            input = input.to(device)\r\n",
        "            target = target.to(device)\r\n",
        "            # reset the hidden states if some sessions have just terminated\r\n",
        "            hidden = reset_hidden(hidden, mask).detach()\r\n",
        "            # Go through the GRU layer\r\n",
        "            logit, hidden = self.gru(input, target, hidden)\r\n",
        "            # Output sampling\r\n",
        "            logit_sampled = logit[:, target.view(-1)]\r\n",
        "            # Calculate the mini-batch loss\r\n",
        "            loss = self.loss_fn(logit_sampled)\r\n",
        "            with torch.no_grad():\r\n",
        "                recall, precision,mrr = evaluate(logit, target, k)\r\n",
        "            losses.append(loss.item())         \r\n",
        "            recalls.append(recall)\r\n",
        "            precisions.append(precision)\r\n",
        "            mrrs.append(mrr)\r\n",
        "            # Gradient Clipping(Optional)\r\n",
        "            if self.clip_grad != -1:\r\n",
        "                for p in self.gru.parameters():\r\n",
        "                    p.grad.data.clamp_(max=self.clip_grad)\r\n",
        "            # Mini-batch GD\r\n",
        "            if training:\r\n",
        "                # Backprop\r\n",
        "                loss.backward()\r\n",
        "                optimizer.step()\r\n",
        "                optimizer.zero_grad() # flush the gradient after the optimization\r\n",
        "\r\n",
        "        results = dict()\r\n",
        "        results['loss'] = np.mean(losses)\r\n",
        "        results['recall'] = np.mean(recalls)\r\n",
        "        results['precision'] = np.mean(precisions)\r\n",
        "        results['mrr'] = np.mean(mrrs)\r\n",
        "        \r\n",
        "        end_time = time.time()\r\n",
        "        results['time'] = (end_time - start_time) / 60\r\n",
        "        \r\n",
        "        if not training:\r\n",
        "            self.gru.train()\r\n",
        "\r\n",
        "        return results\r\n",
        "    \r\n",
        "    \r\n",
        "    def train(self, dataset, k=20, n_epochs=10, save_dir='./models', save=True, model_name='GRU4REC'):\r\n",
        "        \"\"\"\r\n",
        "        Train the GRU4REC model on a pandas dataframe for several training epochs,\r\n",
        "        and store the intermediate models to the user-specified directory.\r\n",
        "        Args:\r\n",
        "            n_epochs (int): the number of training epochs to run\r\n",
        "            save_dir (str): the path to save the intermediate trained models\r\n",
        "            model_name (str): name of the model\r\n",
        "        \"\"\"\r\n",
        "        print(f'Training {model_name}...')\r\n",
        "        for epoch in range(n_epochs):\r\n",
        "            results = self.run_epoch(dataset, k=k, training=True)\r\n",
        "            results = [f'{k}:{v:.3f}' for k, v in results.items()]\r\n",
        "            print(f'epoch:{epoch+1:2d}/{\"/\".join(results)}')\r\n",
        "            \r\n",
        "            # Store the intermediate model\r\n",
        "            if save:\r\n",
        "                save_dir = Path(save_dir)\r\n",
        "                if not save_dir.exists(): save_dir.mkdir()\r\n",
        "                model_fname = f'{model_name}_{self.loss_type}_{self.optimizer_type}_{self.lr}_epoch{epoch+1:d}'\r\n",
        "                torch.save(self.gru.state_dict(), save_dir/model_fname)\r\n",
        "    \r\n",
        "\r\n",
        "    def test(self, dataset, k=20):\r\n",
        "        \"\"\" Model evaluation\r\n",
        "        Args:\r\n",
        "            k (int): the length of the recommendation list\r\n",
        "        Returns:\r\n",
        "            avg_loss: mean of the losses over the session-parallel minibatches\r\n",
        "            avg_recall: mean of the Recall@K over the session-parallel mini-batches\r\n",
        "            avg_precision: mean of the Precision@K over the session-parallel mini-batches\r\n",
        "            avg_mrr: mean of the MRR@K over the session-parallel mini-batches\r\n",
        "            wall_clock: time took for testing\r\n",
        "        \"\"\" \r\n",
        "        results = self.run_epoch(dataset, k=k, training=False)\r\n",
        "        results = [f'{k}:{v:.3f}' for k, v in results.items()]\r\n",
        "        print(f'Test result: {\"/\".join(results)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai1BF4ehfiU4"
      },
      "source": [
        "train_dataset = SessionDataset('./recommendation_dataset/user_train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cwmi47uYoKnl",
        "outputId": "6237ed3c-bd5c-44de-b468-a363c57bf863"
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.SessionDataset at 0x7efd1d700210>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_CuYCxNkH96",
        "outputId": "3ef5f287-7302-460c-ce24-e14cc7278261"
      },
      "source": [
        "input_size = len(train_dataset.items)\r\n",
        "hidden_size = 100\r\n",
        "num_layers = 1\r\n",
        "output_size = input_size\r\n",
        "batch_size = 50\r\n",
        "\r\n",
        "optimizer_type = 'Adagrad'\r\n",
        "lr = .01\r\n",
        "weight_decay = 0\r\n",
        "momentum = 0\r\n",
        "eps = 1e-6\r\n",
        "\r\n",
        "loss_type = 'TOP1-max'\r\n",
        "\r\n",
        "n_epochs = 10\r\n",
        "use_cuda = True\r\n",
        "\r\n",
        "torch.manual_seed(7)\r\n",
        "\r\n",
        "model = GRU4REC(input_size, hidden_size, output_size,\r\n",
        "                num_layers=num_layers,\r\n",
        "                batch_size=batch_size,\r\n",
        "                optimizer_type=optimizer_type,\r\n",
        "                lr=lr,\r\n",
        "                weight_decay=weight_decay,\r\n",
        "                momentum=momentum,\r\n",
        "                eps=eps,\r\n",
        "                loss_type=loss_type,\r\n",
        "                use_cuda=use_cuda)\r\n",
        "\r\n",
        "model_name = 'GRU4REC'\r\n",
        "model.train(train_dataset, n_epochs=n_epochs, model_name=model_name, save=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training GRU4REC...\n",
            "epoch: 1/loss:0.020/recall:0.115/precision:0.289/mrr:0.089/time:0.040\n",
            "epoch: 2/loss:0.019/recall:0.299/precision:0.748/mrr:0.227/time:0.041\n",
            "epoch: 3/loss:0.019/recall:0.396/precision:0.990/mrr:0.301/time:0.041\n",
            "epoch: 4/loss:0.018/recall:0.457/precision:1.141/mrr:0.344/time:0.040\n",
            "epoch: 5/loss:0.018/recall:0.508/precision:1.269/mrr:0.374/time:0.040\n",
            "epoch: 6/loss:0.018/recall:0.550/precision:1.376/mrr:0.399/time:0.040\n",
            "epoch: 7/loss:0.018/recall:0.591/precision:1.478/mrr:0.422/time:0.040\n",
            "epoch: 8/loss:0.018/recall:0.626/precision:1.564/mrr:0.442/time:0.040\n",
            "epoch: 9/loss:0.018/recall:0.657/precision:1.643/mrr:0.459/time:0.040\n",
            "epoch:10/loss:0.018/recall:0.682/precision:1.704/mrr:0.475/time:0.040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQqNOB5Un3Tz"
      },
      "source": [
        "test_dataset = SessionDataset('./recommendation_dataset/user_test.csv', itemmap=train_dataset.itemmap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7hVCwmxn738",
        "outputId": "ac5f0255-0029-44dd-95d7-533d9e454f58"
      },
      "source": [
        "k = 20\r\n",
        "model.test(test_dataset, k=k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test result: loss:0.019/recall:0.247/precision:0.617/mrr:0.169/time:0.007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIvF6kwJubGP"
      },
      "source": [
        "# input_size = len(train_dataset.items)\r\n",
        "# hidden_size = 100\r\n",
        "# num_layers = 1\r\n",
        "# output_size = input_size\r\n",
        "# batch_size = 50\r\n",
        "\r\n",
        "# optimizer_type = 'Adagrad'\r\n",
        "# lr = .01\r\n",
        "# weight_decay = 0\r\n",
        "# momentum = 0\r\n",
        "# eps = 1e-6\r\n",
        "\r\n",
        "# loss_type = 'BPR'\r\n",
        "\r\n",
        "# n_epochs = 10\r\n",
        "# use_cuda = True\r\n",
        "\r\n",
        "# torch.manual_seed(7)\r\n",
        "\r\n",
        "# model = GRU4REC(input_size, hidden_size, output_size,\r\n",
        "#                 num_layers=num_layers,\r\n",
        "#                 batch_size=batch_size,\r\n",
        "#                 optimizer_type=optimizer_type,\r\n",
        "#                 lr=lr,\r\n",
        "#                 weight_decay=weight_decay,\r\n",
        "#                 momentum=momentum,\r\n",
        "#                 eps=eps,\r\n",
        "#                 loss_type=loss_type,\r\n",
        "#                 use_cuda=use_cuda)\r\n",
        "\r\n",
        "# model_name = 'GRU4REC'\r\n",
        "# model.train(train_dataset, n_epochs=n_epochs, model_name=model_name, save=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3x8izbLOwhr-",
        "outputId": "0858cc9f-de5c-4e2a-ecfe-9d80aacec9b6"
      },
      "source": [
        "k = 20\r\n",
        "model.test(test_dataset, k=k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test result: loss:1.002/recall:0.076/mrr:0.042/time:0.026\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}